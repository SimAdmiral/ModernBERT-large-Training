# trainig
# !pip install transformers datasets seqeval evaluate torch accelerate
import json
import numpy as np
import evaluate
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    TrainingArguments,
    Trainer,
    DataCollatorForTokenClassification
)

# ------------------------------------------------------------------
# 1. CONFIGURATION
# ------------------------------------------------------------------
MODEL_CHECKPOINT = "answerdotai/ModernBERT-large"
DATASET_FILE = "output_ner_dataset.json" # The file generated by the previous script
OUTPUT_DIR = "./modernbert-cti-bioul"
MAX_SEQ_LEN = 1024  # ModernBERT supports 8192, but 1024 is usually sufficient for NER/efficiency
BATCH_SIZE = 4      # ModernBERT-large is VRAM heavy; reduce to 2 or 1 if OOM
EPOCHS = 4
LEARNING_RATE = 2e-5

# ------------------------------------------------------------------
# 2. STRICT LABEL MAPPING (MUST MATCH PREVIOUS SCRIPT)
# ------------------------------------------------------------------
LABEL_TO_ID = {
    "O": 0,
    "B-URL": 1, "I-URL": 2, "L-URL": 3, "U-URL": 4,
    "B-MALWARE": 5, "I-MALWARE": 6, "L-MALWARE": 7, "U-MALWARE": 8,
    "B-MITRE_TACTIC": 9, "I-MITRE_TACTIC": 10, "L-MITRE_TACTIC": 11, "U-MITRE_TACTIC": 12,
    "B-MITRE_TECHNIQUE": 13, "I-MITRE_TECHNIQUE": 14, "L-MITRE_TECHNIQUE": 15, "U-MITRE_TECHNIQUE": 16,
    "B-CTI_GROUP": 17, "I-CTI_GROUP": 18, "L-CTI_GROUP": 19, "U-CTI_GROUP": 20,
    "B-CTI_CAMPAIGN": 21, "I-CTI_CAMPAIGN": 22, "L-CTI_CAMPAIGN": 23, "U-CTI_CAMPAIGN": 24,
    "B-TOOL": 25, "I-TOOL": 26, "L-TOOL": 27, "U-TOOL": 28,
    "B-DOMAIN": 29, "I-DOMAIN": 30, "L-DOMAIN": 31, "U-DOMAIN": 32
}

# Create reverse mapping for metrics
ID_TO_LABEL = {v: k for k, v in LABEL_TO_ID.items()}
LABEL_LIST = list(LABEL_TO_ID.keys()) # For seqeval

# ------------------------------------------------------------------
# 3. DATA PREPARATION
# ------------------------------------------------------------------
def main():
    print(f"Loading tokenizer: {MODEL_CHECKPOINT}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)

    # The dataset loader expects the file path
    print(f"Loading dataset from {DATASET_FILE}...")
    raw_datasets = load_dataset("json", data_files=DATASET_FILE)
    
    # Split into train/validation (90/10 split)
    split_datasets = raw_datasets["train"].train_test_split(test_size=0.1, seed=42)

    def preprocess_function(examples):
        """
        Reconstructs input_ids from the pre-tokenized 'tokens' list
        and adds special tokens ([CLS]/[SEP]) and label masks (-100).
        """
        batch_input_ids = []
        batch_labels = []

        for tokens, tags in zip(examples["tokens"], examples["ner_tags"]):
            # 1. Convert subword strings back to IDs
            # Note: The input 'tokens' are already subwords from the previous script
            ids = tokenizer.convert_tokens_to_ids(tokens)
            
            # 2. Add Special Tokens (CLS at start, SEP at end)
            # ModernBERT handles this via cls_token_id/sep_token_id
            input_ids = [tokenizer.cls_token_id] + ids + [tokenizer.sep_token_id]
            
            # 3. Create Labels (Pad special tokens with -100)
            # -100 is the ignore_index for CrossEntropyLoss in PyTorch
            labels = [-100] + tags + [-100]

            # 4. Truncation (if necessary)
            if len(input_ids) > MAX_SEQ_LEN:
                input_ids = input_ids[:MAX_SEQ_LEN]
                labels = labels[:MAX_SEQ_LEN]
                # Ensure the last token is SEP if we truncated hard (optional but good practice)
                input_ids[-1] = tokenizer.sep_token_id
                labels[-1] = -100

            batch_input_ids.append(input_ids)
            batch_labels.append(labels)

        return {
            "input_ids": batch_input_ids,
            "labels": batch_labels,
            # Attention mask is handled automatically by DataCollator, 
            # but we can explicit it here if we want.
            "attention_mask": [[1] * len(ids) for ids in batch_input_ids]
        }

    print("Preprocessing dataset...")
    tokenized_datasets = split_datasets.map(
        preprocess_function,
        batched=True,
        remove_columns=split_datasets["train"].column_names
    )

    # ------------------------------------------------------------------
    # 4. METRICS
    # ------------------------------------------------------------------
    seqeval = evaluate.load("seqeval")

    def compute_metrics(p):
        predictions, labels = p
        predictions = np.argmax(predictions, axis=2)

        true_predictions = []
        true_labels = []

        for prediction, label in zip(predictions, labels):
            true_predictions_row = []
            true_labels_row = []
            for p_id, l_id in zip(prediction, label):
                if l_id != -100:
                    true_predictions_row.append(ID_TO_LABEL[p_id])
                    true_labels_row.append(ID_TO_LABEL[l_id])
            true_predictions.append(true_predictions_row)
            true_labels.append(true_labels_row)

        results = seqeval.compute(predictions=true_predictions, references=true_labels)
        return {
            "precision": results["overall_precision"],
            "recall": results["overall_recall"],
            "f1": results["overall_f1"],
            "accuracy": results["overall_accuracy"],
        }

    # ------------------------------------------------------------------
    # 5. MODEL & TRAINER
    # ------------------------------------------------------------------
    print("Initializing Model...")
    model = AutoModelForTokenClassification.from_pretrained(
        MODEL_CHECKPOINT,
        num_labels=len(LABEL_TO_ID),
        id2label=ID_TO_LABEL,
        label2id=LABEL_TO_ID
    )

    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

    args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=LEARNING_RATE,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        num_train_epochs=EPOCHS,
        weight_decay=0.01,
        bf16=True, # ModernBERT works best with BFloat16 on Ampere+ GPUs
        logging_steps=10,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        push_to_hub=False
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    print("Starting Training...")
    trainer.train()

    print(f"Saving model to {OUTPUT_DIR}...")
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    # Save the label mapping specifically for inference scripts to use easily
    with open(f"{OUTPUT_DIR}/label_map.json", "w") as f:
        json.dump(LABEL_TO_ID, f, indent=2)

if __name__ == "__main__":
    main()